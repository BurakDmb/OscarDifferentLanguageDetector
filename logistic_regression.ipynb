{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/15 23:44:03 WARN Utils: Your hostname, bhdemirbilek resolves to a loopback address: 127.0.1.1; using 10.1.46.97 instead (on interface eno1)\n",
      "22/01/15 23:44:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/bhdemirbilek/miniconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/01/15 23:44:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\").setAppName(\"CENG790-Project\")\n",
    "conf.set(\"spark.driver.memory\", \"15g\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Saving the oscar dataset(28GB) to json format. (only execute once)\n",
    "# from datasets import load_from_disk\n",
    "# dataset = load_from_disk(\"lang_detected\")[\"train\"]\n",
    "# # Set num_proc according to your cpu count, num_proc=20 means 20 thread will be executed paralelly.\n",
    "# dataset.to_json(\"dataset_json\", num_proc=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Reads from the dataset_json.json file, \n",
    "df_json = spark.read.json(\"dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+\n",
      "| id|lang|                text|\n",
      "+---+----+--------------------+\n",
      "|  0|  tr|Son yıllarda görü...|\n",
      "|  1|  tr|Şehrin karmaşası ...|\n",
      "|  2|  tr|2010 Yılında Mard...|\n",
      "|  3|  tr|29Ekim 2009 2010 ...|\n",
      "|  4|  tr|Yüksek İslam Şura...|\n",
      "|  5|  tr|Oncelıkle bu etkı...|\n",
      "|  6|  tr|Mavi-Mi Sanat Mer...|\n",
      "|  7|  tr|Türkiye Futbol Fe...|\n",
      "|  8|  tr|anlami-nedir.com'...|\n",
      "|  9|  tr|Kepez Belediye Ba...|\n",
      "+---+----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:==================================>                   (181 + 24) / 285]\r"
     ]
    }
   ],
   "source": [
    "small_df = df_json.limit(10000)\n",
    "small_df.show(10)\n",
    "small_df.write.mode(\"overwrite\").json(\"dataset_small.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df2 = spark.read.json(\"dataset_small.json\").select(\"lang\", \"text\")\n",
    "#small_df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(small_df2)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledDataWithLang = idfModel.transform(featurizedData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaledDataWithLang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"lang\", outputCol=\"label\")\n",
    "rescaledData = indexer.fit(rescaledDataWithLang).transform(rescaledDataWithLang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rescaledData.count())\n",
    "rescaledData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaledData.select(\"features\", \"label\").write.mode(\"overwrite\").json(\"dataset_small_rescaled.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "\n",
    "schema = StructType([StructField('features', VectorUDT(),False), StructField('label', DoubleType(),False)])\n",
    "\n",
    "rescaledData = spark.read.schema(schema=schema).json(\"dataset_small_rescaled.json\")\n",
    "rescaledData.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaledData.select(\"label\", \"features\").show(1)\n",
    "rescaledData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = rescaledData.select(\"label\", \"features\").randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = spark \\\n",
    "    .read \\\n",
    "    .format(\"libsvm\") \\\n",
    "    .load(\"sample_multiclass_classification_data.txt\")\n",
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=100, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coefficients and intercept for multinomial logistic regression\n",
    "print(\"Coefficients: \\n\" + str(lrModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lrModel.interceptVector))\n",
    "\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)\n",
    "\n",
    "# for multiclass, we can inspect metrics on a per-label basis\n",
    "print(\"False positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"True positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"Precision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "print(\"Recall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "print(\"F-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i, f))\n",
    "\n",
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f4343353fbec2fc34c82a1b09b4d6d1875d4968c8401340e52b24b1aadc6ad7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pyspark': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
